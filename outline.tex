\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\usepackage{bm}
\usepackage{upgreek}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows}

\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%-------------------------------
%	TITLE SECTION
%-------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{
\normalfont \normalsize
\textsc{Brigham Young University} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Practical High Speed Obstacle Avoidance in Unmanned Arieal Vehicals\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{James Jackson% <-this % stops a space
\thanks{James Jackson is with the Department of Mechanical Engineering, Brigham Young University
        {\tt\small jamesjackson@byu.edu}}%
        , Robert Pottorff
\thanks{Robert Pottorff is with the Department of Computer Science, Brigham Young University
        {\tt\small rpottorff@gmail.com}}%
}

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

% \abstract{Practical applications of UAVs are often limited by their ability to react to obstacles at high speeds. Although much progress has been made over the last few decades, current obstacle avoidance methods are either too slow or rely on unreasonable assumptions for some high-speed applications. Leaning on incredible recent successes applying deep reinforcement learning to high dimensional tasks, our agent consists of a deep actor-critic neural network trained using a combination of supervised and reinforcement learning technqiues. We successfully control an agent that can fly up to 72km/h over various simulated obstacle-ridden terrain using only raw sensor data as input. Using inter-network statistics as a component of the cost function, we also demonstrate the successful transfer of learning from networks trained on simualted data, to agents running on real-world data and thereby achieve state-of-the-art results in real-world trials.}

\section{Introduction}

\begin{itemize}
	\item Obstacle avoidance is an essential component of a safe operation of an autonomus agent.
	\item At high speeds obstacle avoidance is hard, and hasn't worked well so far
\end{itemize}
	
\begin{itemize}
	\item Obstacle avoidance is a mapping of sensor inputs and all previous states to a limited available action space
	\item Considering all the sensor data available at high speeds is intractable, but could become tractable given good abstraction
	\item What is the correct high-level space to appropriately handle obstacle avoidance?
\end{itemize}
	
\begin{itemize}
	\item Historically, most obstacle avoidance algorithms have relied on hand-crafted, closed-form rules applied to given fixed transformations of the sensor space such as SIFT, SURF, or ORB keypoints [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=790410]
	\item More recently, some methods have leveraged recent work in deep learning to apply high-level abstractions learned from object recongition tasks [image classification] to improve results but maintain a standard post-feature processing pipeline.
	\item Few of these methods have shown robustness to sensor choice, particularly at high speeds, and none to our knowlege have studied the impact of specialized high level abstractions learned exclusively for obstacle avoidance.
\end{itemize}

\begin{itemize}	
	\item Our motivation in this work is to show that the right high-dimensional feature space can enable an agent to perform with greater accuracy and at higher speeds than previous methods.
	\item We apply advancements made in reinforcement learning to actively learn high-level representations that maximize performance on obstacle avoidance tasks, enabling us to achieve state-of-the-art performance at high speeds in both simulated and real-world scenarios. 
\end{itemize}

\begin{itemize}
	\item Critical to reinforcement learning methods is the capcity to learn via trial and error, often a serious problem for real-world agents where failure is catostrophic.
	\item To overcome this problem, we also introduce a method for combining real world and simulated data.
	\item The success of reinforcement learning to this domain points the way to a generalized algorithm for high speed control that is robust to sensor choice.
\end{itemize}

\section{Background}

	\subsection{Traditional Obstacle Avoidance}
	\begin{itemize}
		\item Most obstacle avoidance algorigthms have focused on designing a closed-form avoidance rule given specific inputs.
		\item FFLOA~\cite{Scherer2007} - first ROAP, uses a heavy, specialized sensor, awesome results.
		\item Schopferer~\cite{Schopferer2014} - closest to optimal ROAP.  Considers kinematic feasibility of avoidane - What sensor did it use?
		\item Oleynikova~\cite{Oleynikova2015} - Stereo vision FFLOA
		\item Saunders~\cite{Saunders2009} - Local memory Voxel Grid-based planning
		\item Richter et al.~\cite{Richter2014} - Path planning based on learned probabilities of collision and optimizing a dynamic model
		\item Jackson~\cite{CEPA} - aimed at reducing limitations imposed by traditional algorithms
		\item none of these algorithms are fast enough to navigate a forest at 72km/h
		\item Expensive map creation
		\item dependence on SLAM
		\item assume nominal forward velocity
	\end{itemize}

	\subsection{Reinforcement Learning}
	\begin{itemize}
		\item reinforcement learning is the process of learning a control policy to an agent by way of a reward signal
		\item this is in contrast to supervised learning where a function is approximated using pairs of input and output signals for example: \cite{Ross2013}
		\item formally this can be constructed as maximizing discounted total future reward over the execution of the policy
		\item \[math\]
		\item optimal action-value functions follow the form of Bellman equation
		\item \[math\]
		\item generally, reinforcement learning algorithms estimate $Q*$ using iterative update [math] \cite{Sutton:1998:IRL:551283}
		\item this is impractical for high dimensional feature spaces, so recently deep neural networks have been used to approximate $Q*$ \cite{Mnih2013}
		\item applications of reinforcement learning to robotic motion in \cite{DBLP:conf/icra/KohlS04,DBLP:conf/iros/TedrakeZS04,geng2005fast,bagnell2001autonomous, riedmiller2009reinforcement}
		\item a more complete survey of reinforcement learning in robotics can be seen in \cite{kober2013reinforcement}
	\end{itemize}

	\subsection{Deep Neural Networks}
	\begin{itemize}
		\item neural networks are a general function approximator that use alternating layers of linear and non-linear operations with parameters that are optimized with respect to a cost function
		\item in context of obstacle avoidance, neural networks have been used in ALVINN \cite{Pomerleau1989,Michels2005,riedmiller2009reinforcement}
		\item recently, a special type of neural network called a convolutional neural networks \cite{DBLP:journals/corr/SzegedyLJSRAEVR14} has vastly improved performance in many vision-based tasks
		\item convolutional neural networks have layers that convolve learned filters across the input producing spatially-aware activations
		\item because of their performance, convnets are extremely common in vision applications
		\item successes include classifying obstacles \cite{Hadsell2009} 
		\item and extracting features for obstacle avoidance
		\item and improving existing algorithms \cite{Kim2015}
		\item most recently, \cite{Guisti2016} could follow an unknown trail in a forrested area by training a DNN to minimize control error relative to an expert data set
	\end{itemize}

	\subsection{Transfer Learning}
	\begin{itemize}
		\item Deep learning has so many parameters, needs tons of data AND reinforcement learning needs failure therefore simulation is practically only option
		\item Simulation is cheap, but comes at the expense of realism
		\item Transfering from simulation to real-life is difficult
		\item transfer learning is the process of transfering knowlege embeded in one network into another network
		\item in our case we want to transfer learning from simulation to the real world
		\item unlike~\cite{Michels2005}, true transferred learning augments networks trained on real-world data with knowledge embedded from a network trained in simulation, rather than an ensemble-like method.
		\item in the context of reinforcement learning, recent work has been done with policy distilation \cite{Rusu2015} and progressive networks \cite{Rusu2016}
		\item These techniques have demonstrated successful demonstrations of transferred learning
		\item A new method, which we introduce here, applies techniques originally developed for style transfer to agent policy~\cite{Gatys2015}
	\end{itemize}

	\subsection{Application of Style Transfer to Policy Transfer}
	\begin{itemize}
		\item recent work has shown gram matricies to be effective at transfering style between images \cite{Gatys2015}
		\item this idea is simple, encourage neurons to fire in a similar pattern between networks
		\item we apply a similar technique to transfer high-level abstractions learned in simulation to networks trained with expert data
	\end{itemize}


\section{Approach} 
\begin{itemize}
	\item what is simulator A?
	\begin{itemize}
		\item built in gazebo
		\item obstacles are pylons and walls randomly placed
		\item second order dynamics for pitch/roll/yaw
		\item first order dynamics for thrust
		\item colision calculation
		\item world is guarnteed to be traversable and meet constraints so that turning radius at n mph is at least x, and \cite{Richter2014}-like 
	\end{itemize}

	\item what is simulator B?
	\begin{itemize}
		\item same as A, but higher fidelity and slower
		\item obstacles are simulated to look like indoor cluttered environments
	\end{itemize}

	\item what does our network look like?
	\begin{itemize}
		\item identical structure to \cite{Lillicrap2015} using additonal techniques of prioritized replay [citation needed], etc.
		\item hyperparameters
		\item topology
		\item picture
	\end{itemize}

	\item how do we train an agent to succeed in supervised simulator A?
	\begin{itemize}
		\item similar to \cite{Kim2015} except with RRT in lieu of expert pilot 
		\item generate 10m training tuples of (sensor, goal trajectory, expected avoidance trajectory) over 10m (world, optimal trajectory)
		\item account for orientation bias in training tuples and real-life noise in sensor data
		\item cost function
		\item network details
	\end{itemize}

	\item how do we train an agent to succeed in unsupervised (RL) simulator A?
	\begin{itemize}
		\item similar to \cite{Lillicrap2015} except with pre-trained network and UAV task instead of whatever they use
		\item new worlds generated every episode similar to supervised worlds
		\item cost function / q learning math
		\item terminal state is defined as 5 meter radius to goal
		\item discritized reward every 10 meters vs change in inverse distance
		\item why discritized reward?
		\item negative reward on crash / death
	\end{itemize}

	\item how do we transfer knowlege from network A to network B
	\begin{itemize}
		\item network A (trained for simulation) has identical structure to network B (trained for real-world)
		\item train network A using simulation
		\item gather data for network B
		\item train network B with additional cost of minimizing gram matrix similarity
	\end{itemize}

\end{itemize}


\section{Results}

\begin{itemize}
	\item Simulator A Results
	\begin{itemize}
		\item How well does the network perform after supervised learning perform? See figure~\ref{fig:prob_completion_vs_epoch}
		\item How well does the network perform after reinforcement learning? See figure~\ref{fig:prob_completion_vs_epoch}
		\item How well does sup+reinforce work? See figure~\ref{fig:prob_completion_vs_epoch}
		\begin{figure}
			\includegraphics[scale=0.5]{fig/prop_of_completion_vs_epoch.png}
			\caption{probability of completion versus training epoch}
			\label{fig:prob_completion_vs_epoch}
		\end{figure}
	\end{itemize}

	\item Transfer Learning
	\begin{itemize}
	    \item What was the impact of the Reinforcement learning phase of Simulator B after transfer?
		\item how many fewer epochs were required with transferred learning?
		\item Convolution Filters Between Networks
		\item How similar are the policies?  What are the differences?
		\item Compare style transfer to Policy Distillation and Progressive Networks
	\end{itemize}

	\item Network Tuning
	\begin{itemize}
		\item What did we have to do to make the network learn?
		\item Comparison of Topologies
		\item Comparison of Hyper-Parameters
		\item Time to Convergence
	\end{itemize}

	\item Simulator B Results
	\begin{itemize}
		\item How well did it ultimately perform?
		\item Discussion of Speed (we went really fast)
	\end{itemize}

\end{itemize}

\section{Conclusion}

\begin{itemize}
	\item It works
	\item We went fast
	\item We learned some stuff about how style transfer can be used to transfer learning
	\item We learned how to transfer learning on robotic simulations
	\item We want to try it in real life
	\item There are other things we want to try too
\end{itemize}


\section{Appendix}


\bibliography{./library}
\bibliographystyle{ieeetr}

\end{document}